{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code for concatenate tx and rx data with same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX data shape: (3756, 8, 8)\n",
      "RX data shape: (3756, 8, 8)\n",
      "All tx data merged and saved to: F:\\My File\\OCC_Nov_11\\Datasets_L8_8_subcarrier\\Datasets\\test_tx_data.npy\n",
      "All rx data merged and saved to: F:\\My File\\OCC_Nov_11\\Datasets_L8_8_subcarrier\\Datasets\\test_rx_noisy_data.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to read data from a file and convert it into a numpy array\n",
    "def read_file_to_array(file_path, dtype):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read().strip().split(',')\n",
    "        return np.array([dtype(i) for i in data])\n",
    "\n",
    "# Directories containing tx and rx data\n",
    "tx_directory = r'F:\\My File\\OCC_Nov_11\\Datasets_L8_8_subcarrier\\5_test_TX_message_I128'\n",
    "rx_directory = r'F:\\My File\\OCC_Nov_11\\Datasets_L8_8_subcarrier\\7_test_rx_decoded_data'\n",
    "\n",
    "# Output paths for merged files\n",
    "output_tx_file = r'F:\\My File\\OCC_Nov_11\\Datasets_L8_8_subcarrier\\Datasets\\test_tx_data.npy'\n",
    "output_rx_file = r'F:\\My File\\OCC_Nov_11\\Datasets_L8_8_subcarrier\\Datasets\\test_rx_noisy_data.npy'\n",
    "\n",
    "# Initialize lists to hold all tx and rx data\n",
    "all_tx_data = []\n",
    "all_rx_data = []\n",
    "\n",
    "# Iterate through all tx files and corresponding rx subfolders and files\n",
    "for file_name in sorted(os.listdir(tx_directory)):\n",
    "    if file_name.endswith('.txt'):  # Ensure we're processing .txt files in the tx folder\n",
    "        tx_file_path = os.path.join(tx_directory, file_name)\n",
    "        \n",
    "        # Find corresponding folder in the rx_directory with the same name as tx file (without extension)\n",
    "        corresponding_rx_folder = os.path.join(rx_directory, file_name.split('.')[0])\n",
    "        \n",
    "        if os.path.exists(corresponding_rx_folder):  # Ensure the corresponding rx folder exists\n",
    "            # Count the number of .txt files in the corresponding rx folder\n",
    "            num_rx_files = len([f for f in os.listdir(corresponding_rx_folder) if f.endswith('.txt')])\n",
    "            # Print the folder name and number of txt files inside\n",
    "            #print(f\"Folder: {corresponding_rx_folder}, Number of .txt files: {num_rx_files}\")\n",
    "\n",
    "            # Iterate through all .txt files in the corresponding rx folder\n",
    "            for rx_file_name in sorted(os.listdir(corresponding_rx_folder)):\n",
    "                if rx_file_name.endswith('.txt'):\n",
    "                    rx_file_path = os.path.join(corresponding_rx_folder, rx_file_name)\n",
    "                    \n",
    "                    # Read tx and rx data\n",
    "                    tx_data = read_file_to_array(tx_file_path, int)\n",
    "                    rx_data = read_file_to_array(rx_file_path, float)\n",
    "                    \n",
    "                    # Append the tx and rx data to lists\n",
    "                    all_tx_data.append(tx_data)\n",
    "                    all_rx_data.append(rx_data)\n",
    "                    \n",
    "                    # Print progress for each file\n",
    "                    #print(f\"Processed {len(tx_data)}/{num_rx_files}\")\n",
    "\n",
    "# Concatenate all tx and rx data into single arrays\n",
    "concatenated_tx_data = np.concatenate(all_tx_data)\n",
    "concatenated_rx_data = np.concatenate(all_rx_data)\n",
    "\n",
    "# Ensure both TX and RX data have the same length before reshaping\n",
    "min_length = min(len(concatenated_tx_data), len(concatenated_rx_data))\n",
    "concatenated_tx_data = concatenated_tx_data[:min_length]\n",
    "concatenated_rx_data = concatenated_rx_data[:min_length]\n",
    "\n",
    "# Reshape the concatenated data into (n, 16) where n is the largest multiple of 16\n",
    "reshaped_tx_data = concatenated_tx_data[:min_length].reshape(-1,8, 8)\n",
    "reshaped_rx_data = concatenated_rx_data[:min_length].reshape(-1,8, 8)\n",
    "\\\n",
    "\n",
    "\n",
    "# Save the reshaped tx and rx data to files\n",
    "np.save(output_tx_file, reshaped_tx_data)\n",
    "np.save(output_rx_file, reshaped_rx_data)\n",
    "\n",
    "# Print output paths and shapes\n",
    "print(f\"TX data shape: {reshaped_tx_data.shape}\")\n",
    "print(f\"RX data shape: {reshaped_rx_data.shape}\")\n",
    "print(f\"All tx data merged and saved to: {output_tx_file}\")\n",
    "print(f\"All rx data merged and saved to: {output_rx_file}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clipped extreme data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the data from the text files\n",
    "rx_noisy_data = r'F:\\My File\\OCC_Nov_11\\Datasets_L8_8_subcarrier\\Datasets\\test_rx_noisy_data.npy'\n",
    "output_filee =  r'F:\\My File\\OCC_Nov_11\\Datasets_L8_8_subcarrier\\Datasets\\test_clip_rx_noisy_data.npy'\n",
    "\n",
    "rx_train = np.load(rx_noisy_data)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape data to (-1, 128)\n",
    "rx_train_reshaped = rx_train.reshape(-1, 128)\n",
    "\n",
    "# Clip extreme values in each batch\n",
    "clipped_data = []\n",
    "for batch in rx_train_reshaped:\n",
    "    lower_threshold = np.percentile(batch, 5)\n",
    "    upper_threshold = np.percentile(batch, 95)\n",
    "    batch_clipped = np.clip(batch, lower_threshold, upper_threshold)\n",
    "    clipped_data.append(batch_clipped)\n",
    "\n",
    "clipped_data = np.array(clipped_data).reshape(-1)  # Flatten back to original shape\n",
    "\n",
    "# Load the datasets into numpy arrays (assuming they're comma-separated)\n",
    "clip_rx_noisy_data = clipped_data.reshape(-1,8,8)\n",
    "np.save(output_filee, clip_rx_noisy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19090, 128)\n",
      "(38180, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "rx_noisy_data = r'F:\\My File\\OCC_Nov_11\\Datasets_L8_8_subcarrier\\Datasets\\train_tx_data.npy'\n",
    "clip_rx_noisy_data =  r'F:\\My File\\OCC_Nov_11\\Datasets_L8_8_subcarrier\\Datasets\\train_tx_data.npy'\n",
    "\n",
    "rx_train = np.load(rx_noisy_data)\n",
    "clip_data = np.load(clip_rx_noisy_data)\n",
    "\n",
    "print((rx_train.reshape(-1,128)).shape)\n",
    "print(clip_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complex dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX data shape: (13688, 16)\n",
      "TX data shape: [[3357.         +0.j           15.63139359-21.24451952j\n",
      "   -15.58578644+18.58578644j ...  -20.74303492+18.82542712j\n",
      "   -15.58578644-18.58578644j   15.63139359+21.24451952j]\n",
      " [3347.         +0.j          -13.01327425+17.71987428j\n",
      "    13.58578644-18.38477631j ...  -12.71987428+21.78571562j\n",
      "    13.58578644+18.38477631j  -13.01327425-17.71987428j]\n",
      " [3348.         +0.j          -16.4737145 +19.86990288j\n",
      "    15.55634919+13.17157288j ...   25.31856124+17.07695404j\n",
      "    15.55634919-13.17157288j  -16.4737145 -19.86990288j]\n",
      " ...\n",
      " [3357.         +0.j          -21.25540818+18.25018172j\n",
      "   -18.38477631-16.41421356j ...  -18.25018172-13.98333025j\n",
      "   -18.38477631+16.41421356j  -21.25540818-18.25018172j]\n",
      " [3355.         +0.j           12.02081528-12.02081528j\n",
      "   -19.79898987+16.79898987j ...  -12.02081528+12.02081528j\n",
      "   -19.79898987-16.79898987j   12.02081528+12.02081528j]\n",
      " [3352.         +0.j          -14.18648347+18.80945224j\n",
      "    15.55634919-15.55634919j ...   21.18357115-14.79690878j\n",
      "    15.55634919+15.55634919j  -14.18648347-18.80945224j]]\n",
      "RX data shape: (13688, 16)\n",
      "All tx data merged and saved to: E:\\Minhaz\\iot\\OCC_sep_5\\new_dataset_16_10\\fft_merged_tx4.npy\n",
      "All rx data merged and saved to: E:\\Minhaz\\iot\\OCC_sep_5\\new_dataset_16_10\\fft_merged_rx4.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to read data from a .txt file and convert it into a numpy array\n",
    "def read_file_to_array(file_path, dtype):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read().strip().split(',')\n",
    "        return np.array([dtype(i) for i in data])\n",
    "\n",
    "# Directories containing tx and rx data\n",
    "tx_directory = r'E:\\Minhaz\\iot\\OCC_sep_5\\new_dataset_16_10\\fft1_tx_message'\n",
    "rx_directory = r'E:\\Minhaz\\iot\\OCC_sep_5\\New_dataset_28_10\\rx_decoded_data1px'\n",
    "\n",
    "# Output paths for merged files\n",
    "output_tx_file = r'E:\\Minhaz\\iot\\OCC_sep_5\\new_dataset_16_10\\fft_merged_tx4.npy'\n",
    "output_rx_file = r'E:\\Minhaz\\iot\\OCC_sep_5\\new_dataset_16_10\\fft_merged_rx4.txt'\n",
    "\n",
    "# Initialize lists to hold all tx and rx data\n",
    "all_tx_data = []\n",
    "all_rx_data = []\n",
    "len_tx_data = 0\n",
    "len_rx_data = 0\n",
    "\n",
    "# Iterate through all tx .npy files and corresponding rx subfolders and files\n",
    "for file_name in sorted(os.listdir(tx_directory)):\n",
    "    if file_name.endswith('.npy'):  # Ensure we're processing .npy files in the tx folder\n",
    "        tx_file_path = os.path.join(tx_directory, file_name)\n",
    "        \n",
    "        # Find corresponding folder in the rx_directory with the same name as tx file (without extension)\n",
    "        corresponding_rx_folder = os.path.join(rx_directory, file_name.split('.')[0])\n",
    "        \n",
    "        if os.path.exists(corresponding_rx_folder):  # Ensure the corresponding rx folder exists\n",
    "            # Count the number of .txt files in the corresponding rx folder\n",
    "            num_rx_files = len([f for f in os.listdir(corresponding_rx_folder) if f.endswith('.txt')])\n",
    "            # Print the folder name and number of txt files inside\n",
    "            #print(f\"Folder: {corresponding_rx_folder}, Number of .txt files: {num_rx_files}\")\n",
    "\n",
    "            # Iterate through all .txt files in the corresponding rx folder\n",
    "            for rx_file_name in sorted(os.listdir(corresponding_rx_folder)):\n",
    "                if rx_file_name.endswith('.txt'):\n",
    "                    rx_file_path = os.path.join(corresponding_rx_folder, rx_file_name)\n",
    "                    \n",
    "                    # Read tx data from .npy and rx data from .txt\n",
    "                    tx_data = np.load(tx_file_path)  # Load .npy file\n",
    "                    rx_data = read_file_to_array(rx_file_path, float)  # Load .txt file\n",
    "                    #print(tx_data)\n",
    "                    # Append the tx and rx data to lists\n",
    "                    all_tx_data.append(tx_data)\n",
    "                    all_rx_data.append(rx_data)\n",
    "                    len_tx_data += len(tx_data)\n",
    "                    len_rx_data += len(rx_data)\n",
    "                    \n",
    "                    #print(all_tx_data)\n",
    "                    # Print progress for each file\n",
    "                    #print(f\"Processed  {len_tx_data} = {len_rx_data}:{len(tx_data)} = {len(rx_data)}/{num_rx_files}\")\n",
    "\n",
    "            #print(all_tx_data)\n",
    "\n",
    "# Concatenate all tx and rx data into single arrays\n",
    "concatenated_tx_data = np.concatenate(all_tx_data, axis=0)\n",
    "concatenated_rx_data = np.concatenate(all_rx_data)\n",
    "#print(concatenated_tx_data[1])\n",
    "\n",
    "# # Ensure both TX and RX data have the same length before reshaping\n",
    "# min_length = min(len(concatenated_tx_data), len(concatenated_rx_data))\n",
    "# concatenated_tx_data = concatenated_tx_data[:min_length]\n",
    "# concatenated_rx_data = concatenated_rx_data[:min_length]\n",
    "\n",
    "# Reshape the concatenated data into (n, 16) where n is the largest multiple of 16\n",
    "#reshaped_tx_data = concatenated_tx_data.reshape(-1, 16)\n",
    "#print(reshaped_tx_data.shape)\n",
    "reshaped_rx_data = concatenated_rx_data.reshape(-1, 16)\n",
    "\n",
    "# Save the reshaped tx data to a .npy file and rx data to a .txt file\n",
    "np.save(output_tx_file, concatenated_tx_data)\n",
    "np.savetxt(output_rx_file, reshaped_rx_data, fmt='%.4f', delimiter=',')\n",
    "\n",
    "# Print output paths and shapes\n",
    "print(f\"TX data shape: {concatenated_tx_data.shape}\")\n",
    "print(f\"TX data shape: {concatenated_tx_data}\")\n",
    "print(f\"RX data shape: {reshaped_rx_data.shape}\")\n",
    "print(f\"All tx data merged and saved to: {output_tx_file}\")\n",
    "print(f\"All rx data merged and saved to: {output_rx_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
